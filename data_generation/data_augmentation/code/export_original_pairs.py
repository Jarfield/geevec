"""ä»åŸå§‹å…ƒæ•°æ®ç›´æ¥æ„å»ºè®­ç»ƒä¸‰å…ƒç»„æ–‡ä»¶ã€‚

è¾“å…¥ï¼šåŸå§‹ corpusã€queries ä¸ qrelsï¼ˆscore=1ï¼‰æ–‡ä»¶ï¼Œå­—æ®µåå¯é€šè¿‡å‚æ•°è¦†ç›–ã€‚
è¾“å‡ºï¼šå½¢å¦‚ ``{"prompt": str, "query": str, "pos": [str], "neg": []}`` çš„ JSONLï¼Œ
      é»˜è®¤å†™å…¥ ``DATA_AUG_ORIGINAL_ROOT/<task>/original_pairs/<language>_original.jsonl``ã€‚
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from typing import Dict, Iterable, List, Optional, Set

from datasets import Dataset, load_dataset
from tqdm import tqdm

THIS_DIR = os.path.dirname(__file__)
PROJECT_ROOT = os.path.abspath(os.path.join(THIS_DIR, "..", "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from data_generation.shared.constants import Language, TaskType, get_task
from data_generation.data_preparation.code.task_configs import (
    DEFAULT_ORIGINAL_ROOT,
    TaskDatasetConfig,
    get_task_config,
)


def _load_dataset(path: str) -> Dataset:
    """æŒ‰ç…§æ‰©å±•ååŠ è½½æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒ jsonl ä¸ Arrowã€‚"""

    if path.endswith(".jsonl"):
        return load_dataset("json", data_files=path)["train"]
    return Dataset.from_file(path)


def _to_lookup(
    ds: Iterable[dict],
    id_key: str,
    text_key: str,
    title_key: Optional[str],
    min_len: int,
) -> Dict[str, str]:
    """å°† Dataset è½¬æˆ id -> æ–‡æœ¬ çš„æŸ¥æ‰¾è¡¨ã€‚"""

    lookup: Dict[str, str] = {}
    for row in tqdm(ds, desc="Loading records"):
        rid = row.get(id_key)
        text = row.get(text_key)
        title = row.get(title_key) if title_key else None

        if text is None:
            continue
        merged_text = f"{title}\n{text}" if title else text
        if len(merged_text.strip()) < min_len:
            continue

        lookup[str(rid)] = merged_text
    return lookup


def _load_positive_map(
    ds: Iterable[dict],
    qid_key: str,
    pid_key: str,
    score_key: str,
    min_positive: int,
) -> Dict[str, Set[str]]:
    """è¯»å– qrelsï¼Œæ”¶é›†æ»¡è¶³åˆ†æ•°é˜ˆå€¼çš„ query->doc æ˜ å°„ã€‚"""

    pos_map: Dict[str, Set[str]] = {}
    for row in tqdm(ds, desc="Loading qrels"):
        try:
            score = int(row.get(score_key, 0))
        except Exception:
            continue
        if score < min_positive:
            continue

        qid = str(row.get(qid_key))
        pid = str(row.get(pid_key))
        if qid not in pos_map:
            pos_map[qid] = set()
        pos_map[qid].add(pid)
    return pos_map


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Convert metadata into training-ready triplets")

    parser.add_argument("--task_type", required=True, choices=[t.name for t in TaskType], help="ä»»åŠ¡å")
    parser.add_argument("--language", default="en", choices=[l.name for l in Language], help="è¯­è¨€ä»£ç ")

    parser.add_argument("--corpus_path", default=None, help="è‡ªå®šä¹‰ corpus è·¯å¾„ï¼Œé»˜è®¤è¯»å– task_configs")
    parser.add_argument("--queries_path", default=None, help="è‡ªå®šä¹‰ queries è·¯å¾„ï¼Œé»˜è®¤è¯»å– task_configs")
    parser.add_argument("--qrels_path", default=None, help="è‡ªå®šä¹‰ qrels è·¯å¾„ï¼Œé»˜è®¤è¯»å– task_configs")

    parser.add_argument("--corpus_id_key", default=None, help="corpus ä¸»é”®å­—æ®µåï¼Œé»˜è®¤å– task_configs.id_key")
    parser.add_argument("--corpus_text_key", default=None, help="corpus æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤å– task_configs.text_key")
    parser.add_argument("--corpus_title_key", default=None, help="corpus æ ‡é¢˜å­—æ®µåï¼Œå¯é€‰ï¼Œå­˜åœ¨æ—¶ä¼šä¸æ­£æ–‡æ‹¼æ¥")
    parser.add_argument("--query_id_key", default=None, help="queries ä¸»é”®å­—æ®µåï¼Œé»˜è®¤å– task_configs.query_id_key")
    parser.add_argument("--query_text_key", default=None, help="queries æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤å– task_configs.query_text_key")
    parser.add_argument("--qrels_qid_key", default=None, help="qrels ä¸­ query id å­—æ®µåï¼Œé»˜è®¤å– task_configs.qrels_qid_key")
    parser.add_argument("--qrels_pid_key", default=None, help="qrels ä¸­ doc id å­—æ®µåï¼Œé»˜è®¤å– task_configs.qrels_pid_key")
    parser.add_argument("--qrels_score_key", default=None, help="qrels ä¸­åˆ†æ•°å­—æ®µåï¼Œé»˜è®¤å– task_configs.qrels_score_key")

    parser.add_argument("--positive_score", type=int, default=1, help="è®¤ä¸ºæ˜¯æ­£ä¾‹çš„æœ€å° score é˜ˆå€¼ï¼Œé»˜è®¤ 1")
    parser.add_argument("--min_len", type=int, default=None, help="è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬çš„æœ€å°é•¿åº¦ï¼Œé»˜è®¤å– task_configs.min_len")
    parser.add_argument("--max_queries", type=int, default=-1, help="æœ€å¤šä¿ç•™å¤šå°‘æ¡ queryï¼Œ-1 è¡¨ç¤ºä¸è¿‡æ»¤")

    parser.add_argument("--save_root", default=None, help="è¾“å‡ºæ ¹ç›®å½•ï¼Œé»˜è®¤ DATA_AUG_ORIGINAL_ROOT")
    parser.add_argument(
        "--output_path",
        default=None,
        help="å®Œæ•´è¾“å‡ºè·¯å¾„ï¼›è‹¥æä¾›åˆ™å¿½ç•¥ save_root/language ç»„åˆæ–‡ä»¶å",
    )
    parser.add_argument("--overwrite", action="store_true", help="è‹¥æ–‡ä»¶å­˜åœ¨æ˜¯å¦è¦†ç›–")

    return parser.parse_args()


def resolve_paths(args: argparse.Namespace, cfg: TaskDatasetConfig) -> tuple[str, str, str]:
    corpus_path = args.corpus_path or cfg.corpus_path
    queries_path = args.queries_path or cfg.queries_path
    qrels_path = args.qrels_path or cfg.qrels_path

    if corpus_path is None:
        raise ValueError("corpus_path æœªæä¾›ï¼Œè¯·é€šè¿‡å‚æ•°æˆ– task_configs é…ç½®")
    if queries_path is None:
        raise ValueError("queries_path æœªæä¾›ï¼Œè¯·é€šè¿‡å‚æ•°æˆ– task_configs é…ç½®")
    if qrels_path is None:
        raise ValueError("qrels_path æœªæä¾›ï¼Œè¯·é€šè¿‡å‚æ•°æˆ– task_configs é…ç½®")
    return corpus_path, queries_path, qrels_path


def main():
    args = parse_args()

    cfg = get_task_config(args.task_type)
    corpus_path, queries_path, qrels_path = resolve_paths(args, cfg)

    corpus_id_key = args.corpus_id_key or cfg.id_key
    corpus_text_key = args.corpus_text_key or cfg.text_key
    corpus_title_key = args.corpus_title_key or cfg.title_key
    query_id_key = args.query_id_key or cfg.query_id_key
    query_text_key = args.query_text_key or cfg.query_text_key
    qrels_qid_key = args.qrels_qid_key or cfg.qrels_qid_key
    qrels_pid_key = args.qrels_pid_key or cfg.qrels_pid_key
    qrels_score_key = args.qrels_score_key or cfg.qrels_score_key
    min_len = args.min_len if args.min_len is not None else cfg.min_len
    
    # ğŸ’¡ 1. è·å– Task å¯¹è±¡å’Œ prompt (task_instruction)
    task_obj = get_task(args.task_type, args.language)
    task_instruction = task_obj.task_instruction
    if task_instruction is None:
        raise ValueError(f"TaskType.{args.task_type} åœ¨ constant ä¸­æœªå®šä¹‰ä»»åŠ¡è¯´æ˜ (value)")

    # ---------- åŠ è½½æ•°æ® ----------
    corpus_ds = _load_dataset(corpus_path)
    queries_ds = _load_dataset(queries_path)
    qrels_ds = _load_dataset(qrels_path)

    corpus_lookup = _to_lookup(
        corpus_ds,
        id_key=corpus_id_key,
        text_key=corpus_text_key,
        title_key=corpus_title_key,
        min_len=min_len,
    )
    query_lookup = _to_lookup(
        queries_ds,
        id_key=query_id_key,
        text_key=query_text_key,
        title_key=None,
        min_len=0,
    )
    pos_map = _load_positive_map(
        qrels_ds,
        qid_key=qrels_qid_key,
        pid_key=qrels_pid_key,
        score_key=qrels_score_key,
        min_positive=args.positive_score,
    )

    print(f"[INFO] Loaded corpus docs: {len(corpus_lookup)} | queries: {len(query_lookup)} | qrels pairs: {len(pos_map)}")

    # ---------- ç»„è£…è¾“å‡º ----------
    results: List[dict] = []
    for idx, (qid, doc_ids) in enumerate(pos_map.items()):
        if 0 <= args.max_queries <= idx:
            break
        query_text = query_lookup.get(qid)
        if not query_text:
            continue
        pos_texts = [corpus_lookup[pid] for pid in doc_ids if pid in corpus_lookup]
        if not pos_texts:
            continue
        # ğŸ’¡ 2. å°† task_instruction èµ‹å€¼ç»™ "prompt" å­—æ®µ
        results.append(
            {
                "prompt": task_instruction, 
                "query": query_text,
                "pos": sorted(set(pos_texts)),
                "neg": [],
            }
        )

    save_root = args.save_root or DEFAULT_ORIGINAL_ROOT
    if args.output_path:
        save_path = args.output_path
    else:
        save_dir = os.path.join(save_root, args.task_type, "original_pairs")
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, f"{args.language}_original.jsonl")

    if os.path.exists(save_path) and not args.overwrite:
        raise FileExistsError(f"æ–‡ä»¶å·²å­˜åœ¨ä¸”æœªæŒ‡å®š --overwrite: {save_path}")

    with open(save_path, "w", encoding="utf-8") as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"[INFO] Saved {len(results)} records to {save_path}")


if __name__ == "__main__":
    main()
