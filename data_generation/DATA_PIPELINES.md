# Data augmentation pipelines for key retrieval tasks

This document summarises the end-to-end data construction pipelines for the four core retrieval tasks used in this repo: **AILAStatutes**, **ArguAna**, **CovidRetrieval**, and **SCIDOCS**. Each pipeline follows the same two-stage structure—synthesising documents from positive seeds and then generating (query, positive) training pairs—with task-specific inputs and narrative controls highlighted below.

## Shared building blocks
- **Seed loading and filtering**: `CorpusGenerator` reads the task-specific corpus and optional qrels, discarding short texts and keeping only positive ids when qrels are provided. It supports Arrow and JSONL corpora and optional subsampling via `--num_seed_samples`.【F:data_generation/data_augmentation/code/corpus_generator.py†L1-L48】【F:data_generation/data_augmentation/code/run_corpus_generation.py†L137-L175】
- **Document synthesis**: `DocSynthesisGenerator` rewrites each seed into one or more stylistic variants, sampling narrative attributes that are specialised per task (e.g., legal style for AILAStatutes, argumentative tone for ArguAna). The generator cleans the LLM output into `title` and `text` fields and preserves the seed id and sampled attributes.【F:data_generation/data_augmentation/code/doc_synthesis_generator.py†L64-L178】【F:data_generation/data_augmentation/code/attributes_config.py†L13-L76】
- **Query generation**: `run_generation.py` loads the synthesised corpus (default path: `<DATA_AUG_GENERATED_ROOT>/<task>/generation_results/generated_corpus/<lang>_synth_corpus.jsonl`) and feeds each document to `TripletGenerator`, optionally with few-shot examples. It writes deduplicated triplets per language/task and supports multi-round generation with narrative focuses for selected tasks.【F:data_generation/data_augmentation/code/run_generation.py†L26-L588】
- **Task prompts**: `get_generation_prompt` defines the instruction per task (e.g., “generate a situation” for statutes, “generate a claim” for ArguAna) and inserts optional focus hints used in multi-round modes.【F:data_generation/data_augmentation/code/constant.py†L62-L180】
- **Dataset defaults**: `task_configs.py` centralises corpus/qrels paths, id/text field names, minimum length filters, and example directories for all tasks. These defaults can be overridden via CLI flags in the runner scripts.【F:data_generation/data_augmentation/code/task_configs.py†L16-L169】

## AILAStatutes pipeline
1. **Seed selection**: Load the MTEB AILA Statutes corpus and qrels (`aila_statutes-corpus.arrow`, `aila_statutes-test.arrow`), keeping documents longer than 200 characters and with positive qrels ids when available.【F:data_generation/data_augmentation/code/task_configs.py†L145-L167】【F:data_generation/data_augmentation/code/corpus_generator.py†L8-L48】
2. **Statute rewriting**: Run `run_corpus_generation.py --task_type ailastatutes` to create stylised legal descriptions. `DocSynthesisGenerator` samples legal-specific genres (e.g., “法律条文式叙述”), formal tone, and structured bulleting from the task’s attribute options before rewriting each statute.【F:data_generation/data_augmentation/code/run_corpus_generation.py†L121-L175】【F:data_generation/data_augmentation/code/attributes_config.py†L55-L77】
3. **Situation generation**: Invoke `run_generation.py --task_type ailastatutes` on the synthesised corpus. For multi-round runs, the script cycles through narrative focuses—victim, investigation, judgment, social impact, and neutral brief—so each statute spawns diverse situations across rounds; single-round runs skip focus control. Each output triplet stores the generated situation as `query` and the statute text as `pos`.【F:data_generation/data_augmentation/code/run_generation.py†L459-L588】【F:data_generation/data_augmentation/code/constant.py†L75-L99】

## ArguAna pipeline
1. **Seed selection**: Read the ArguAna JSONL corpus and test qrels; keep passages with positive labels and length ≥200 characters. Paths default to the MTEB snapshot under `shared_models/datasets--mteb--arguana`.【F:data_generation/data_augmentation/code/task_configs.py†L131-L144】【F:data_generation/data_augmentation/code/corpus_generator.py†L8-L48】
2. **Argument rewriting**: Use `run_corpus_generation.py --task_type arguana` to rewrite argumentative passages. The attribute sampler adds debate-style genres and “立场鲜明” tones to diversify outputs while preserving argumentative structure.【F:data_generation/data_augmentation/code/doc_synthesis_generator.py†L64-L178】【F:data_generation/data_augmentation/code/attributes_config.py†L55-L76】
3. **Claim generation**: Run `run_generation.py --task_type arguana` against the synthetic corpus. The task prompt instructs the model to generate claims that the passage would refute, producing triplets of `{query: claim, pos: passage}`. ArguAna uses the single-round path by default (no narrative focus cycling).【F:data_generation/data_augmentation/code/run_generation.py†L446-L520】【F:data_generation/data_augmentation/code/constant.py†L75-L99】

## CovidRetrieval pipeline
1. **Seed selection**: Load the COVID news corpus and dev qrels from the C-MTEB snapshot, keeping texts over 200 characters and restricting to positive qrels ids. This ensures seeds are topical to COVID-19 questions.【F:data_generation/data_augmentation/code/task_configs.py†L85-L107】【F:data_generation/data_augmentation/code/corpus_generator.py†L8-L48】
2. **News rewriting**: Execute `run_corpus_generation.py --task_type covidretrieval` to synthesise varied news-style articles. The attribute sampler augments base attributes with pandemic-specific genres and tones (e.g., “疫情通报”, “防疫政策解读”) to broaden coverage.【F:data_generation/data_augmentation/code/run_corpus_generation.py†L121-L175】【F:data_generation/data_augmentation/code/attributes_config.py†L55-L76】
3. **Question generation with focused rounds**: Call `run_generation.py --task_type covidretrieval --num_rounds N`. In multi-round mode, the script cycles through five focus hints—factual details, policies, vaccines/treatments, risk/protection, and social impact—each steering `get_generation_prompt` to ask a different COVID-related question type per document. Results are saved per round (e.g., `en-triplets_round1.jsonl`) with deduplication across runs.【F:data_generation/data_augmentation/code/run_generation.py†L520-L588】【F:data_generation/data_augmentation/code/constant.py†L103-L139】

## SCIDOCS pipeline
1. **Seed selection**: Pull SCIDOCS abstracts and test qrels from the MTEB snapshot, respecting the `_id` and `text` fields and the 200-character minimum. Qrels limit seeds to cited/positive pairs when provided.【F:data_generation/data_augmentation/code/task_configs.py†L108-L130】【F:data_generation/data_augmentation/code/corpus_generator.py†L8-L48】
2. **Abstract rewriting**: Run `run_corpus_generation.py --task_type scidocs` to produce stylistically varied abstracts. The attribute sampler injects academic genres and audiences (e.g., “学术摘要风格”, “面向研究人员”) to diversify phrasing while retaining content.【F:data_generation/data_augmentation/code/doc_synthesis_generator.py†L64-L178】【F:data_generation/data_augmentation/code/attributes_config.py†L55-L76】
3. **Title generation**: Use `run_generation.py --task_type scidocs` on the synthetic abstracts. The generation prompt asks for paper titles likely to cite the abstract, yielding triplets where the title is the query and the rewritten abstract is the positive passage; SCIDOCS uses the default single-round path without narrative focuses.【F:data_generation/data_augmentation/code/run_generation.py†L446-L520】【F:data_generation/data_augmentation/code/constant.py†L75-L99】
